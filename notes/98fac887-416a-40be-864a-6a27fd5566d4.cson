createdAt: "2019-10-15T09:59:36.238Z"
updatedAt: "2019-12-17T23:24:00.632Z"
type: "MARKDOWN_NOTE"
folder: "07413230992e30754bd9"
title: "Finding the root causes of accidents"
tags: [
  "CM30072"
  "Safety-Critical_Systems"
  "Computer_Science"
  "University"
]
content: '''
  # Finding the root causes of accidents
  
  ## Hierarchical model of accident causes
  ### Level 3: Constraints
  Constraints (or lack of constraints) that allowed conditions to cause events, or allowed to conditions to exist
  
  These are often refered to as root causes
  
  #### Examples
  - Lack of street signs
  - Penalties for not wearing seatbelt 
  - Lack of pedestrian crossings
  - Safety culture
  
  ### Level 2: Conditions
  Conditions (or lack of conditions) that allow the events in level 1 to occur
  
  #### Examples
  - Inexperienced driver
  - No ABS
  - Wet roads (less friction)
  - No (or defective) seatbelt
  - Person crossing the street
  
  ### Level 1: Mechanisms
  Succession of events
  
  #### Examples
  Driver hit the break, the car skidded into a tree, the driver was then thrown from the car and injured
  
  ---
  
  ## Analyzing level 3: constraints
  ### What does it include?
  - Constraints on the technical and physical conditions
  - Social dynamics/human actions
  - Management system and organisational culture
  - Govermental and socioeconomic policies
  
  These causes are most likely affect future accidents, however we often pay more attention to levels 1 and 2
  
  ---
   
  ## Finding root causes of accidents
  Finding the root cause of an accident is key to preventing similar accidents.
  
  The root level causes of accidents can be divided into three categories:
  1. Deficiencies in the safety culture of the industry or organisation
  2. Flawed organisational structures
  3. Superficial or ineffective technical activities
  
  ### Deficiencies in the safety culture of the industry or organisation
  **Safety culture** - The general attitude and approach to safety reflected by those who participate in the industry
  e.g workers, management, industry regulators, and government regulators
  
  Ideally participants are all equally concerned about safety, both in the processes they use and in the final product
  
  However this is not always the case, this is why we have industry and government regulators
  
  #### The deficiencies in safety culture
  - Overconfidence and complacency
    - Discounting risk
    - Over-reliance on redundancy
    - Unrealistic risk assessment
    - Ignoring high-consequence, low probability events
    - Assuming risk decreases over time
    - Underestimating software related risks
    - Ignoring warning signs
  - Disregard or low priority for safety
  - Flaws resolution of conflicting goals
  
  #### Overconfidence and complacency
  ##### Discounting risk
  Major accidents are often preceded by the belief that they cannot happen
  
  A good example of this is the Titanic:
  Nobody thought it could sink, and because of this a lot of safety protocols were glossed over or disregarded all together
  
  ###### The Titanic effect:
  >Used to explain the fact that
  major accidents are often
  preceded by a belief that they
  cannot happen.
  
  ##### Over-reliance on redundancy
  Redundant systems use extra components to ensure that the failure of one component doesn't result in the failure of the whole system
  
  Many accidents can be traced back to "common-cause failures" in redundant systems
  
  **Common-cause failure** - these occure when multiple redundant components fail at the same time, for the same reason
  For example in the case of a fire or electrical outage
  
  Therefore we must be aware that although redundancy may help if one component fails, but we must be aware that all components can fail
  
  ###### Challanger space shuttle
  This had a redundant system for the rocket boosters, however both rings failed at once
  
  ##### Unrealistic risk assessment
  It is common for developers to overestimate how resilient a system is and make unrealistic risk assesments
  This can lead to a lack of investigation into how it actually performs
  
  ###### Therac-25 
  The software risk assessment was 10<sup>-11</sup> chance that the computer selects the wrong energy level
  This pretty much meant that accidents were impossible
  Because of this there were no investigations around possible overdoses
  
  This resulted in 6 accidents between 1985 and 1987 with doses approximetly 100 times the intended dose
  
  ##### Ignoring high-consiquence, low probability events
  A common discovery after accidents is that the events involved were regarded as very hazardous before the accident, but were dismissed as increadible
  
  > risk = severity x probability
  
  ###### Apollo 13
  Ground control did not believe what their instruments were telling them
  There were no pre scenarios where two oxygen tanks and two fuel cells were damaged at the same time
  They thought it could not happen
  
  ##### Assuming risk decreases over time
  There is a common belief that a system must be fine because it operated without any accidents for many years
  
  Risk may decrease, remain constant, or increase over time
  
  It depends on the system
  
  It can increase due to operators becoming over-familiar with safety procedures and hence becoming lax or even missing them out
  
  ###### Therac-25
  This system operated thousands of times before the first accident. With time, operators began to use the system faster, triggering a software error that had not surfaced before
  
  ##### Underestimating software related risks
  There is a pervading belief that software cannot fail, and that all errors can be removed by testing
  
  Hardware backups, interlocks, etc, are being replaced with software
  
  Many of these mechanical safety devices are well tested, cheap, reliable, and failsafe (based on physical principles to fail in a safe state)
  
  Most industries that are run with computers are not computer scientists
  People that are not in the software industry often thing that systems cannot fail
  
  It is therefore misguided to replace the mechanical safety devices with software alternatives
  
  A similar argument can be made for changing existing software, where some changes can appear easy, there are often more factors that need to be taken into account
  
  ###### Mars climate orbiter
  The software was changed to include a new thruster equation, but the 4.45 correction factor (the difference between the metric and imperial units), blured in the origional code was not noticed when the new vendor-supplied equation was used to update the software
  
  ##### Ignoring warning signs
  Accidents are frequently preceded by public warnings or a series of minor occurences
  
  ###### Kings cross underground fire
  This was apparently caused by a lighted match which droped into an escalator, which fell through and set fire to dust and grease on the track beneath
  
  There had actually been an average of 20 fires a year from 1958 to 1987, which were called "smoulderings" presumably to make them sound less serious.
  They caused some damage but nobody was killed
  This may have lead to the views that these fires were not dangerous
  
  ##### Disregard or low priority for safety
  Problems occur if management is not interested in safety
  Workers will not be encouraged to think about safety
  
  The Government may disregard safety and ignore the need for government/industry watchdogs and standards comittees
  These comittees often only appear after major accidents
  
  To prevent this:
  - The organisation must have a high level of commitment to safety in order to prevent accidents
  - The lead must come from the top and permiate ever organizational level
  
  ###### Clapham Junction rail crash (1988)
  Train ran rear to another stationary train
  After the impact the first train struck an oncoming train
  Although the report stated that there was sincere concern for safety, no action was taken
  
  ##### Flawed resolution of confliction goals
  Occurs if there are deficiencies in the safety culture of an organisation/industry
  
  The most common example is the "cost-safety" tradoff:
  > Safety costs money
  
  Or it at least appears to cost money at the time of development/manufacture
  
  Often cost becomes more important and safety may therefore be compramised in the ruch for greater profits
  
  ### Flawed organisational structures
  Many accident investigations find sincere concern for safety in an organisation, but the orginational structures in place were ineffective in implementing concern
  
  #### Diffuction of responsibility and authority
  Accidents are often associated with ill-defined responsibility and authority for safety matters
  There should be at least one person with overall responsibility for safety, these people should have real power within the organisation
  
  #### Lack of independence and low status of stafety personnel
  This leads to their inability or unwillingness to bring up safety issues
  Safety officers should not be under supervision of the groups whos activities theu must check
  Low status means that there is no involvement in decision making
  
  #### Poor and limited communication channels
  In some industries, strict line management means that workers report only to their direct superiors
  Problems with safety may not be reported to the interested parties and will not be acted upon
  All staff should have direct access to the safety personnel and visa versa
  
  > Companies do not factor the potential cossts of catastrophies when making decisions, therefore there is often insufficient resources allocated to managing safety
  
  Once this is understood, it clears the way for initiatives which can dramatically reduce the risk of catastrophes:
  - Giving those closest the problem the decision-making power to respond
  - Improving internal communications
  - Encouraging employees to come foward with 'bad news'
  - Not overworking employees
  - Providing special training to alert workers to potential dangers
  - Making sure that sophisticated technology does not diminish a workers ability to assess a situation
  
  #### Herald of Free Enterprise (1987)
  This ferry sank moments after leaving the Belgian port of Zeebrugge
  - It had been designed for rapid loading and unloading on the competitive cross-channel route
  - 193 passsengers and creq died
  - There was negligence from the the seaman through to the board of directors
    - Seaman responsible for closing the door was asleep
    - Officer responsible for monitoring the door-closing said he saw someone perpering to close them
    - Captain followed the standing order which saidd that if no problems were reported assume that the vessel is ready for sea
    - There was pressure on crew to cut turnaround time
    - As the ferry was optimised for rapid loading it sunk in 90 seconds giving no time to evacuate passangers
    - Owner company renamed to P&O
    - Official inquary placed more blame on his supervisors and a general culture of poor communication in the ferry company
  
  ### Superficial or ineffective technical activities
  This is concerned with poor implementation of all the activities necessary to achieve an acceptable level of safety
  
  #### Superficial safety efforts
  - Hazard logs kept but no description of design decisions taken or trade-offs made to mitigate/control the recognised hazards
  - No follow-ups to ensure that hazards have ever been controlled 
  - No follow-ups to ensure safety devices are kept in working order
  
  #### Ineffective risk control
  - Know risks but don't control them
  - The majority of risks are **not** the results of a lack of knowledge of how to prevent them
    - They are results of the failure to use the knowledge effectively when trying to fix the problems
  
  #### Failure to evaluate changes
  - Accidents often involve a failure to re-evaluate safety after changes are made
  - Any changes in hardware or software **MUST** be re-evaluated to determine whether safety has been comprimised
  - Quick fixes often affect safety because they are not evaluated properly
  - In the case of software
    - Regression test
    - System and software safety analysis
  
  #### Information deficiencies
  - Feedback of operational experience is one of the most important sorces of designing, maintaining and improving safety
    - This is often overlooked
  - There are two types of data that are important
    - Information about accidents/incidents for the system itself
    - Information about accidents/incidents for similar systems
  - Case studies provide information to affirm/diffirm hypotheses and intutions an to learn the mistakes not to repeat
  
  ---
  
  ## Conclusion
  Root causes of accidents are broad
  They are rooted in the company where they happened
  They are not usually to do with the components failing or operator error
  The level of management and design is usually to blame
  
  > To reduce the risk of accidents, root causes must be identified and eliminated
  > If we are unable to iliminate a particular casual factor, we still may be able to provide protection against it leading to an accident
'''
linesHighlighted: [
  97
  108
  118
  130
  147
]
isStarred: false
isTrashed: false
